{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dog & Cat CNNClassification.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQwdHfQ5BuUO"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "\n",
        "# Data pre-processing and data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "\t\trotation_range = 4.0,\n",
        "\t\twidth_shift_range = 0.2,\n",
        "\t\theight_shift_range = 0.2,\n",
        "\t\tshear_range = 0.2,\n",
        "\t\tzoom_range = 0.2,\n",
        "\t\thorizontal_flip = True,\n",
        "\t\tfill_mode = 'nearest')\n",
        "\n",
        "img = load_img('/content/sample_data/train/cats/cat.0.jpg') # this is a PIL image\n",
        "x = img_to_array(img) # this is a Numpy array with shape (3, 150, 150)\n",
        "x = x. reshape((1,) + x.shape) # this is a Numpy array with shape (1, 3, 150, 150)\n",
        "\n",
        "# the .flow() command below generates batches of randomly transformed images\n",
        "# and saves the results to the `preview/` directory\n",
        "i = 0\n",
        "for batch in datagen.flow(x, batch_size = 1,\n",
        "                          save_to_dir = 'sample_data/preview', save_prefix='cat', save_format='jpeg'):\n",
        "\t\ti += 1\n",
        "\t\tif i > 20:\n",
        "\t\t\tbreak; # otherwise generator would loop indefinately\n",
        "\n",
        "\n",
        "# My First CNN Model for Dogs vs Cats Classification\n",
        "# Training a small convnet from scratch: 80% accuracy in 40 lines of code\n",
        "# The right tool for an image classification job is a convnet i.e CNN\n",
        "# The main focus for fighting overfitting should be the entropic \n",
        "# capacity of the model, how much information the model is allowed to store. \n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import Activation, Dropout, Flatten, Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3,3), input_shape=(3, 150, 150), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,1)))\n",
        "\n",
        "model.add(Conv2D(32, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# the model so far outputs 3D feature maps (height, width, features)\n",
        "\n",
        "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "model.add(Dense(64))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# Let's prepare our data. We will use .flow_from_directory()\n",
        "# to generate batches of image data (and their labels) directly\n",
        "# from our jpgs in their respective folders.\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "# this is the augmentation configuration we will use for training\n",
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True)\n",
        "\n",
        "# this is the augmentation configuration we will use for testing:\n",
        "# only rescaling\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# this is a generator that will read pictures found in\n",
        "# subfolers of 'data/train', and indefinitely generate\n",
        "# batches of augmented image data\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        'sample_data/train',  # this is the target directory\n",
        "        target_size=(150, 150),  # all images will be resized to 150x150\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary')  # since we use binary_crossentropy loss, we need binary labels\n",
        "\n",
        "# this is a similar generator, for validation data\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "        'sample_data/validation',\n",
        "        target_size=(150, 150),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary')\n"
      ]
    }
  ]
}